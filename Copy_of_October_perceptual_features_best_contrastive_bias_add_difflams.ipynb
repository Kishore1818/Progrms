{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N4DDwdwLnKp",
        "outputId": "e88f6ec5-f310-4d38-afa3-7b3d1ee0e073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive', force_remount=True) # My Drive\n",
        "# Change to your local path if needed\n",
        " \n",
        "path = '/content/gdrive/My Drive'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.image as img\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torchvision.models as models\n",
        "\n"
      ],
      "metadata": {
        "id": "_7h7irhDZA5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_contrastive_trans(feats,label_feats,thresh=2,tau=0.25,eps=1e-7):\n",
        "    #this assumes that label_feats contain relevant feature vectors.\n",
        "    #feats are expected to be of size (batch_size x feat_length)\n",
        "\n",
        "    # pos_ix = (labels == 1).nonzero()\n",
        "    # neg_ix = (labels == 0).nonzero()\n",
        "    # pos_feats = feats[pos_ix]\n",
        "    # neg_feats = feats[neg_ix]\n",
        "\n",
        "    #perform masking and thresholding on perceptual features\n",
        "    #reshape\n",
        "    label_feats1 = torch.reshape(label_feats,(label_feats.shape[0],1,-1))\n",
        "    label_feats2 = torch.reshape(label_feats,(1,label_feats.shape[0],-1))\n",
        "\n",
        "    dist = torch.mean(torch.abs(label_feats1-label_feats2),dim=-1)\n",
        "    # print(torch.mean(dist))\n",
        "    # print('*************')\n",
        "    # print(dist)\n",
        "\n",
        "    mask_same = (dist<thresh).type(torch.uint8) \n",
        "    mask_diff = (dist>=thresh).type(torch.uint8) \n",
        "\n",
        "    #reshape all for broadcasting\n",
        "    feats1 = torch.reshape(feats,(feats.shape[0],1,-1))\n",
        "    feats2 = torch.reshape(feats,(1,feats.shape[0],-1))\n",
        "\n",
        "    dot = torch.sum(torch.abs(feats1-feats2),dim=-1)\n",
        "    sim = dot#/(pos_norm1*pos_norm2)\n",
        "    met = sim#torch.exp(pos_sim/tau)\n",
        "\n",
        "    num = torch.sum(mask_same*met)\n",
        "    den = torch.sum(mask_diff*met)\n",
        "\n",
        "    # #reshape for broadcasting\n",
        "    # pos_feats1 = torch.reshape(pos_feats,(pos_feats.shape[0],1,-1))\n",
        "    # pos_feats2 = torch.reshape(pos_feats,(1,pos_feats.shape[0],-1))\n",
        "\n",
        "    # neg_feats1 = torch.reshape(neg_feats,(neg_feats.shape[0],1,-1))\n",
        "    # neg_feats2 = torch.reshape(neg_feats,(1,neg_feats.shape[0],-1))\n",
        "\n",
        "    # #compute norms\n",
        "    # pos_norm1 = torch.norm(pos_feats1,p=2,dim=-1)\n",
        "    # pos_norm2 = torch.norm(pos_feats2,p=2,dim=-1)\n",
        "\n",
        "    # neg_norm1 = torch.norm(neg_feats1,p=2,dim=-1)\n",
        "    # neg_norm2 = torch.norm(neg_feats2,p=2,dim=-1)\n",
        "\n",
        "    # #compute positive similarity contrasts\n",
        "    # pos_dot = torch.sum(torch.abs(pos_feats1-pos_feats2),dim=-1)\n",
        "    # pos_sim = pos_dot#/(pos_norm1*pos_norm2)\n",
        "    # pos_met = pos_sim#torch.exp(pos_sim/tau)\n",
        "\n",
        "    # #compute negative similarity contrasts\n",
        "    # neg_dot = torch.sum(torch.abs(neg_feats1-neg_feats2),dim=-1)\n",
        "    # neg_sim = neg_dot#/(neg_norm1*neg_norm2)\n",
        "    # neg_met = neg_sim#torch.exp(neg_sim/tau)\n",
        "\n",
        "    # #compute cross similarity contrasts\n",
        "    # cross_dot = torch.sum(torch.abs(pos_feats1-neg_feats2),dim=-1)\n",
        "    # cross_sim = cross_dot#/(pos_norm1*neg_norm2)\n",
        "    # cross_met = cross_sim#torch.exp(cross_sim/tau)\n",
        "\n",
        "    # #start computing the numerators and denominators\n",
        "    # pos_num = torch.sum(pos_met,dim=-1)\n",
        "    # neg_num = torch.sum(neg_met,dim=-1)\n",
        "\n",
        "    # pos_den = torch.sum(cross_met,dim=-1)\n",
        "    # neg_den = torch.sum(cross_met,dim=0)\n",
        "\n",
        "    # #compute log terms\n",
        "    # pos_log = torch.log(pos_num/(pos_den+eps))\n",
        "    # neg_log = torch.log(neg_num/(neg_den+eps))\n",
        "\n",
        "    # print(pos_log.shape,neg_log.shape,pos_den.shape,neg_den.shape)\n",
        "    # loss = -1*(torch.sum(pos_log)+torch.sum(neg_log))\n",
        "\n",
        "    loss = -1*torch.log(num/(den+eps))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JSJPWBN5ZKpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fix random seed\n",
        "sd = 0\n",
        "np.random.seed(sd)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(sd)\n",
        "random.seed(sd)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(sd)"
      ],
      "metadata": {
        "id": "JSjKAA3hZOqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict to map given label to a number\n",
        "dict_age_to_number = {'0-2': 0,\n",
        "                      '3-9' : 1,\n",
        "                      '10-19' : 2,\n",
        "                      '20-29' : 3,\n",
        "                      '30-39' : 4,\n",
        "                      '40-49' : 5,\n",
        "                      '50-59' : 6,\n",
        "                      '60-69' : 7,\n",
        "                      'more than 70' : 8}\n",
        "\n",
        "dict_gender_to_number = {'Male' : 0, \n",
        "                        'Female': 1}\n",
        "\n",
        "dict_race_to_number = {'Black' : 0,\n",
        "                       'White' : 1}"
      ],
      "metadata": {
        "id": "lSWfybc1ZRdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './UTKFace/'\n",
        "\n",
        "data_labels = os.listdir('UTKFace')\n",
        "\n",
        "clean_labels = []\n",
        "\n",
        "age = []\n",
        "age2 = []\n",
        "gender = []\n",
        "race = []\n",
        "for f in data_labels:\n",
        "    temp = f.split('_')\n",
        "    if len(temp[2])>1:\n",
        "        continue\n",
        "    age.append(temp[0])\n",
        "    age2.append(int(temp[0]))\n",
        "    gender.append(temp[1])\n",
        "    race.append(temp[2])\n",
        "    clean_labels.append(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "oO9dwM5-ZTz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign labels to samples\n",
        "age_class = []\n",
        "for a in age2:\n",
        "    if a<=2 and a>=0:\n",
        "        age_class.append('0-2')\n",
        "    elif a>=3 and a<=9:\n",
        "        age_class.append('3-9')\n",
        "    elif a>=10 and a<=19:\n",
        "        age_class.append('10-19')\n",
        "    elif a>=20 and a<=29:\n",
        "        age_class.append('20-29')\n",
        "    elif a>=30 and a<=39:\n",
        "        age_class.append('30-39')\n",
        "    elif a>=40 and a<=49:\n",
        "        age_class.append('40-49')\n",
        "    elif a>=50 and a<=59:\n",
        "        age_class.append('50-59')\n",
        "    elif a>=60 and a<=69:\n",
        "        age_class.append('60-69')\n",
        "    elif a>=70:\n",
        "        age_class.append('more than 70')\n",
        "    else:\n",
        "        print('ErrorA')\n",
        "        print(a)\n",
        "        \n",
        "gender_class = []\n",
        "for g in gender:\n",
        "    if g=='0':\n",
        "        gender_class.append('Male')\n",
        "    elif g=='1':\n",
        "        gender_class.append('Female')\n",
        "    else:\n",
        "        print('ErrorG')\n",
        "        print(g)\n",
        "        \n",
        "race_class = []\n",
        "for g in race:\n",
        "    if g=='0':\n",
        "        race_class.append('White')\n",
        "    elif g=='1':\n",
        "        race_class.append('Black')\n",
        "    elif g=='2':\n",
        "        race_class.append('Asian')\n",
        "    elif g=='3':\n",
        "        race_class.append('Indian')\n",
        "    elif g=='4':\n",
        "        race_class.append('Other')\n",
        "    else:\n",
        "        print('ErrorR')\n",
        "        print(g)\n",
        "\n"
      ],
      "metadata": {
        "id": "E6uH2ovlZWTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create pandas dataframe\n",
        "df = {'file': clean_labels, 'age': age_class, 'gender': gender_class, 'race': race_class}\n",
        "\n",
        "df = pd.DataFrame(data=df)"
      ],
      "metadata": {
        "id": "I3AWJG_QZZj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categories\n",
        "age_list = ['3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 'more than 70']"
      ],
      "metadata": {
        "id": "E_6oX6kiZhGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = df"
      ],
      "metadata": {
        "id": "GqM0F-thZjGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloader\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, data, path , transform = None):\n",
        "        super().__init__()\n",
        "        self.data = data.values\n",
        "        self.path = path\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        img_name = self.data[index][0]\n",
        "        label = dict_gender_to_number[self.data[index][2]]   # index 3 for race, need as tensor -> convert to number from str first\n",
        "        label = torch.tensor(label)\n",
        "        img_path = os.path.join(self.path, img_name)\n",
        "        image = img.imread(img_path)\n",
        "\n",
        "        #group label\n",
        "        gp_label = dict_race_to_number[self.data[index][3]]\n",
        "        gp_label = torch.tensor(gp_label)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label, gp_label"
      ],
      "metadata": {
        "id": "xId0tByaZmAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transforms go here\n",
        "train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "valid_transform = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "fsRYaMvjZoVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for data resampling\n",
        "def resample_dataset_race(data,frac):\n",
        "    flagsD = data['race']=='Black'\n",
        "    flagsL = data['race']=='White'\n",
        "    data_D = data[flagsD]\n",
        "    data_L = data[flagsL]\n",
        "    \n",
        "    data_D = equalize_dataset_gender(data_D,0.5)\n",
        "    data_L = equalize_dataset_gender(data_L,0.5)\n",
        "    \n",
        "    baseline = min(len(data_D),len(data_L))-2\n",
        "    \n",
        "    data_D = data_D.sort_values('gender')\n",
        "    data_L = data_L.sort_values('gender')\n",
        "    remD = len(data_D)-int((frac)*baseline)\n",
        "    tempD = data_D[int(0.5*remD):-int(0.5*remD)]\n",
        "    remL = len(data_L)-int((1-frac)*baseline)\n",
        "    tempL = data_L[int(0.5*remL):-int(0.5*remL)]\n",
        "    \n",
        "    print(baseline,remD,remL)\n",
        "    \n",
        "    frames = [tempD,tempL]\n",
        "    final_split = pd.concat(frames)\n",
        "    final_split = final_split.sample(frac=1)\n",
        "    \n",
        "    return final_split\n",
        "\n",
        "def resample_dataset_race_old(data,frac):\n",
        "    flagsD = data['race']=='Black'\n",
        "    flagsL = data['race']=='White'\n",
        "    data_D = data[flagsD]\n",
        "    data_L = data[flagsL]\n",
        "    \n",
        "    \n",
        "    baseline = min(len(data_D),len(data_L))\n",
        "    \n",
        "    tempD = data_D[0:int(frac*baseline)]\n",
        "    tempL = data_L[0:int((1-frac)*baseline)]\n",
        "    \n",
        "    frames = [tempD,tempL]\n",
        "    final_split = pd.concat(frames)\n",
        "    final_split = final_split.sample(frac=1)\n",
        "    \n",
        "    return final_split\n",
        "\n",
        "def equalize_dataset_gender(data,frac):\n",
        "    flagsM = data['gender']=='Male'\n",
        "    flagsF = data['gender']=='Female'\n",
        "    data_M = data[flagsM]\n",
        "    data_F = data[flagsF]\n",
        "    baseline = min(len(data_M),len(data_F))\n",
        "    \n",
        "    tempM = data_M[0:int(baseline)]\n",
        "    tempF = data_F[0:int(baseline)]\n",
        "    frames = [tempM,tempF]\n",
        "    final_split = pd.concat(frames)\n",
        "    final_split = final_split.sample(frac=1)\n",
        "    \n",
        "    return final_split\n",
        "\n",
        "\n",
        "#functions for data\n",
        "def resample_dataset_equal(data):\n",
        "    flagsD = data['race']=='Black'\n",
        "    flagsL = data['race']=='White'\n",
        "    data_D = data[flagsD]\n",
        "    data_L = data[flagsL]\n",
        "    \n",
        "    data_D = equalize_dataset_gender(data_D,0.5)\n",
        "    data_L = equalize_dataset_gender(data_L,0.5)\n",
        "    \n",
        "    baseline = min(len(data_D),len(data_L))\n",
        "    \n",
        "    data_D = data_D.sort_values('gender')\n",
        "    data_L = data_L.sort_values('gender')\n",
        "    \n",
        "    remD = len(data_D)-int((0.5)*baseline)\n",
        "    tempD = data_D[int(0.5*remD):-int(0.5*remD)]\n",
        "    remL = len(data_L)-int((0.5)*baseline)\n",
        "    tempL = data_L[int(0.5*remL):-int(0.5*remL)]\n",
        "\n",
        "    return tempD,tempL"
      ],
      "metadata": {
        "id": "OgEHZUoyZtnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split datasets\n",
        "batch_size = 30\n",
        "\n",
        "# def resample_dataset_gender(data,frac):\n",
        "#     flagsM = data['gender']=='Male'\n",
        "#     flagsF = data['gender']=='Female'\n",
        "#     data_M = data[flagsM]\n",
        "#     data_F = data[flagsF]\n",
        "#     baseline = min(len(data_M),len(data_F))\n",
        "    \n",
        "#     tempM = data_M[0:int(frac*baseline)]\n",
        "#     tempF = data_F[0:int((1-frac)*baseline)]\n",
        "#     frames = [tempM,tempF]\n",
        "#     final_split = pd.concat(frames)\n",
        "#     final_split = final_split.sample(frac=1)\n",
        "    \n",
        "#     return final_split\n",
        "\n",
        "\n",
        "train_split_t, val_split_t_dash = train_test_split(train_labels, stratify=train_labels.gender, test_size=0.2)\n",
        "val_split_t, test_labels = train_test_split(val_split_t_dash, stratify=val_split_t_dash.gender, test_size=0.5)\n",
        "\n",
        "#Ensure val set has equal representation\n",
        "flagsD = val_split_t['race']=='Black'\n",
        "flagsL = val_split_t['race']=='White'\n",
        "data_L = val_split_t[flagsL]\n",
        "data_L = equalize_dataset_gender(data_L,0.5)\n",
        "\n",
        "data_D = val_split_t[flagsD]\n",
        "data_D = equalize_dataset_gender(data_D,0.5)\n",
        "\n",
        "val_split = data_L.sample(frac=1) \n",
        "\n",
        "test_split_D,test_split_L = resample_dataset_equal(test_labels)\n",
        "\n",
        "print(val_split['gender'].value_counts())\n",
        "print(val_split['race'].value_counts())\n",
        "\n",
        "print(test_split_D['gender'].value_counts())\n",
        "print(test_split_D['race'].value_counts())\n",
        "\n",
        "print(test_split_L['gender'].value_counts())\n",
        "print(test_split_L['race'].value_counts())\n",
        "\n",
        "#dataloaders\n",
        "valid_data = UTKFaceDataset(val_split, data_path, valid_transform )\n",
        "test_data_D = UTKFaceDataset(test_split_D, data_path, test_transform )\n",
        "test_data_L = UTKFaceDataset(test_split_L, data_path, test_transform )\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size = batch_size, shuffle=False, num_workers=0)\n",
        "test_loader_D = DataLoader(dataset = test_data_D, batch_size = batch_size, shuffle=False, num_workers=0)\n",
        "test_loader_L = DataLoader(dataset = test_data_L, batch_size = batch_size, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "Uvjw6TXZZyfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776f7c7a-5215-4817-b3cb-86d2b1aa4477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female    446\n",
            "Male      446\n",
            "Name: gender, dtype: int64\n",
            "White    892\n",
            "Name: race, dtype: int64\n",
            "Female    112\n",
            "Male      112\n",
            "Name: gender, dtype: int64\n",
            "Black    224\n",
            "Name: race, dtype: int64\n",
            "Female    112\n",
            "Male      112\n",
            "Name: gender, dtype: int64\n",
            "White    224\n",
            "Name: race, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "#Model\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def swish(x):\n",
        "    return F.relu(x)\n",
        "\n",
        "class Network1(nn.Module):\n",
        "\n",
        "    def __init__(self,D_out=2,dtype = torch.FloatTensor,device = 'cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.D_out = D_out\n",
        "\n",
        "        model1 = models.resnet34(pretrained=False)\n",
        "        # model.fc = nn.Linear(512, num_classes)\n",
        "        newmodel1 = torch.nn.Sequential(*(list(model1.children())[:-1]))\n",
        "\n",
        "        model2 = models.resnet34(pretrained=False)\n",
        "        # model2.fc = nn.Linear(512, 512*D_out)\n",
        "        model2.fc = nn.Linear(512, (512+1)*D_out)\n",
        "        \n",
        "\n",
        "        self.head1 = newmodel1\n",
        "        self.head2 = model2\n",
        "\n",
        "        self.w = Variable(torch.randn(1, 512, D_out).type(dtype), requires_grad=True).to(device)\n",
        "        self.b = Variable(torch.randn(1, D_out).type(dtype), requires_grad=True).to(device)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x1 = self.head1(x) ##Feature head\n",
        "        x2 = self.head2(x) ##Offset head\n",
        "\n",
        "        #reshape x1 and x2\n",
        "        x1 = torch.reshape(x1,(x1.shape[0],1,x1.shape[1]))\n",
        "\n",
        "        ttemp = x2[:,512*self.D_out:]\n",
        "\n",
        "        x2 = torch.reshape(x2[:,0:512*self.D_out],(x2.shape[0],x1.shape[2],-1))\n",
        "        # print(x1.shape,x2.shape)\n",
        "        # print(x1-x2)\n",
        "        # print(ttemp.shape)\n",
        "        update_bias = ttemp+torch.tile(self.b, (x1.shape[0],1))\n",
        "\n",
        "        update_wt = x2+torch.tile(self.w, (x1.shape[0],1,1))\n",
        "\n",
        "        x = torch.matmul(x1,update_wt)\n",
        "\n",
        "        x = torch.reshape(x,(x.shape[0],-1))+update_bias#self.b\n",
        "        # print(x.shape,x1.shape,x2.shape)\n",
        "\n",
        "        return x, x1, x2\n",
        "\n",
        "\n",
        "class Network2(nn.Module):\n",
        "\n",
        "    def __init__(self,D_out=2,dtype = torch.FloatTensor,device = 'cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "        model1 = models.resnet34(pretrained=False)\n",
        "        # model.fc = nn.Linear(512, num_classes)\n",
        "        newmodel1 = torch.nn.Sequential(*(list(model1.children())[:-1]))\n",
        "\n",
        "        model2 = models.resnet34(pretrained=False)\n",
        "        model2.fc = nn.Linear(512, 512)\n",
        "        \n",
        "\n",
        "        self.head1 = newmodel1\n",
        "        self.head2 = model2\n",
        "\n",
        "        self.w = Variable(torch.randn(1, 512, D_out).type(dtype), requires_grad=True).to(device)\n",
        "        self.b = Variable(torch.randn(1, D_out).type(dtype), requires_grad=True).to(device)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x1 = self.head1(x) ##Feature head\n",
        "        x2 = self.head2(x) ##Offset head\n",
        "\n",
        "        #reshape x1 and x2\n",
        "        x1 = torch.reshape(x1,(x1.shape[0],1,x1.shape[1]))\n",
        "        x2 = torch.reshape(x2,(x2.shape[0],1,x2.shape[1]))\n",
        "        print(x1.shape,x2.shape)\n",
        "        # print(x1-x2)\n",
        "        update_bias = torch.tile(self.b, (x1.shape[0],1))\n",
        "\n",
        "        update_wt = torch.tile(self.w, (x1.shape[0],1,1))\n",
        "\n",
        "        x = torch.matmul(x1+x2,update_wt)\n",
        "\n",
        "        x = torch.reshape(x,(x.shape[0],-1))+self.b\n",
        "        # print(x.shape,x1.shape,x2.shape)\n",
        "\n",
        "        return x, x1, x2\n",
        "\n",
        "\n",
        "####Feature transformation networks\n",
        "class NetworkFC(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.bn1(F.relu(self.fc1(x)))\n",
        "        x = self.bn2(F.relu(self.fc2(x)))\n",
        "        x = self.bn3(F.relu(self.fc3(x)))\n",
        "        x = F.sigmoid(self.fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NetworkTransform(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 2048)\n",
        "        self.fc3 = nn.Linear(2048, 2048)\n",
        "\n",
        "        self.fc4 = nn.Linear(2048, 2048)\n",
        "        self.fc5 = nn.Linear(2048, 2048)\n",
        "        self.fc6 = nn.Linear(2048, 2048)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "\n",
        "        self.bn4 = nn.BatchNorm1d(2048)\n",
        "        self.bn5 = nn.BatchNorm1d(2048)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x = self.bn1(F.relu(self.fc1(x)))\n",
        "        x = self.bn2(F.relu(self.fc2(x)))\n",
        "        x_temp = self.fc3(x)\n",
        "\n",
        "        x = self.bn4(F.relu(self.fc4(x_temp)))\n",
        "        x = self.bn5(F.relu(self.fc5(x)))\n",
        "        x = self.fc6(x)\n",
        "\n",
        "        return x_temp, x"
      ],
      "metadata": {
        "id": "4QztUjnKaNm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU or GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "VYAUwXgGaTpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b515e4c7-6f0c-454e-f26c-38c235131e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_performance_trans(model,group_disc,model_trans,dataL,criterion):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    test_acc2 = 0\n",
        "    test_acc3 = 0\n",
        "    temp_test_acc = []\n",
        "\n",
        "    for data, target, grp in dataL:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        grp = grp.to(device)\n",
        "\n",
        "        target_dash = torch.zeros((data.shape[0])).to(device)\n",
        "\n",
        "        output_dash = group_disc(data)                    # forward pass\n",
        "        output_dash = torch.reshape(output_dash,(output_dash.shape[0],output_dash.shape[1])).detach()\n",
        "\n",
        "        output_detangle, output_est = model_trans(output_dash)\n",
        "        output = model(output_detangle) \n",
        "\n",
        "        loss = criterion(output.squeeze(-1).float(), target.float())\n",
        "        # update-average-validation-loss \n",
        "        test_loss += loss.item() * data.size(0)\n",
        "\n",
        "        op_temp = output.squeeze(-1).detach().cpu().numpy()\n",
        "        op_temp = (op_temp>0.5).astype(np.uint8)\n",
        "\n",
        "        test_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "        test_acc2 += np.mean(op_temp==target_dash.detach().cpu().numpy())*data.size(0)\n",
        "        test_acc3 += np.mean((output_est.detach().cpu().numpy()-output_dash.detach().cpu().numpy())**2)*data.size(0)\n",
        "\n",
        "    ttacc  = test_acc/len(dataL.sampler)\n",
        "    ttacc2  = test_acc2/len(dataL.sampler)\n",
        "    ttacc3  = test_acc3/len(dataL.sampler)\n",
        "    test_loss_M = test_loss/len(dataL.sampler)\n",
        "    \n",
        "    test_print = 'Test Loss: {:.3f} \\tTest Acc1: {:.3f} \\t2: {:.3f} \\t3: {:.3f}'.format(\n",
        "        test_loss_M, ttacc, ttacc2, ttacc3)\n",
        "\n",
        "    print(test_print)\n",
        "    return test_print,ttacc"
      ],
      "metadata": {
        "id": "x_20a2qsaVrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_file(fname,string,act):\n",
        "    with open(fname, act) as text_file:       \n",
        "        text_file.write(string+'\\n')\n",
        "\n",
        "###Combined iteration\n",
        "def train_model_trans(dark_frac,train_split_t,valid_loader,test_loader_D,test_loader_L):\n",
        "\n",
        "    num_epochs = 60\n",
        "    num_classes = 2 \n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    check_point_dir = 'October_perceptual_features_best_contrastive_bias_add_'+str(dark_frac)\n",
        "    \n",
        "    if not os.path.isdir(f\"checkpoints/\"+check_point_dir):\n",
        "        os.makedirs(f\"checkpoints/\"+check_point_dir)\n",
        "        print(\"Output directory is created\")\n",
        "        \n",
        "    #make logger text file\n",
        "    text_path = f\"checkpoints/\"+check_point_dir+\"/\"+\"log.txt\"\n",
        "    try:\n",
        "        os.remove(text_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    \n",
        "    write_file(text_path,'********* Dark fraction: {} *********'.format(dark_frac),'a')\n",
        "    \n",
        "    train_split = resample_dataset_race(train_split_t,dark_frac)\n",
        "    \n",
        "    write_file(text_path,str(train_split['race'].value_counts()),'a')\n",
        "    \n",
        "    write_file(text_path,str(train_split['gender'].value_counts()),'a')\n",
        "    \n",
        "    #Dataloaders\n",
        "    train_data = UTKFaceDataset(train_split, data_path, train_transform )\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "    group_disc = models.resnet50(pretrained=True)\n",
        "    # group_disc = torch.hub.load(\"pytorch/vision\", \"resnext101_64x4d\", weights=\"IMAGENET1K_V1\")\n",
        "    group_disc = torch.nn.Sequential(*(list(group_disc.children())[:-1]))\n",
        "    # group_disc.fc = nn.Linear(512, 2)\n",
        "\n",
        "    for param in group_disc.parameters():\n",
        "        param.requires_grad = False\n",
        "    group_disc.to(device)\n",
        "\n",
        "    model = NetworkFC()\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    criterion_MSE = nn.MSELoss()\n",
        "\n",
        "    model_trans = NetworkTransform()\n",
        "    model_trans.to(device)\n",
        "\n",
        "    lam1 = 3\n",
        "    lam2 = 3\n",
        "\n",
        "    optimizer_trans = torch.optim.AdamW(\n",
        "        model_trans.parameters(), \n",
        "        lr=learning_rate, \n",
        "        betas=(0.5, 0.999), \n",
        "        weight_decay=0.08\n",
        "        )\n",
        "    \n",
        "    optimizer_disc = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=learning_rate, \n",
        "        betas=(0.5, 0.999), \n",
        "        weight_decay=0.08\n",
        "        )\n",
        "\n",
        "    scheduler_trans = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer_trans, T_max=30, \n",
        "        eta_min=0.01 * learning_rate, verbose=True\n",
        "        )\n",
        "    \n",
        "    scheduler_disc = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer_disc, T_max=30, \n",
        "        eta_min=0.01 * learning_rate, verbose=True\n",
        "        )\n",
        "    \n",
        "    \n",
        "    # Actual training of model\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    valid_accuracy = []\n",
        "    test_accuracy_D = []\n",
        "    test_accuracy_L = []\n",
        "\n",
        "\n",
        "    print(\"Training model...\")\n",
        "\n",
        "    gan_thresh = 0\n",
        "    flg_ctr = 0\n",
        "    flg = 0\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        # keep track of train/val loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ##Set flags\n",
        "\n",
        "        if epoch>gan_thresh:\n",
        "            flg_ctr +=1\n",
        "            if flg_ctr%4==0:\n",
        "                flg = (flg+1)%2\n",
        "                flg_ctr = 0\n",
        "\n",
        "\n",
        "        # training the model\n",
        "        model.train()\n",
        "        group_disc.eval()\n",
        "        temp_train_acc = 0.0\n",
        "        temp_train_acc2 = 0.0\n",
        "        temp_train_acc3 = 0.0\n",
        "        for data, target, grp in train_loader:\n",
        "\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            grp = grp.to(device)\n",
        "\n",
        "            target_dash = torch.zeros((data.shape[0])).to(device)\n",
        "\n",
        "            optimizer_trans.zero_grad()  \n",
        "            optimizer_disc.zero_grad()  \n",
        "\n",
        "            output_dash = group_disc(data)                    # forward pass\n",
        "            output_dash = torch.reshape(output_dash,(output_dash.shape[0],output_dash.shape[1])).detach()\n",
        "\n",
        "            if flg==0: #this means run the generator updates\n",
        "                output_detangle, output_est = model_trans(output_dash)\n",
        "                output = model(output_detangle) \n",
        "\n",
        "                loss = criterion_MSE(output_dash,output_est)+lam1*criterion(output.squeeze(-1).float(), target_dash.float())\n",
        "\n",
        "                loss.backward()                         # loss backwards\n",
        "                optimizer_trans.step()                        # update model params\n",
        "\n",
        "            if flg==1: #this means run the discriminator updates\n",
        "                output_detangle, output_est = model_trans(output_dash)\n",
        "                output = model(output_detangle) \n",
        "\n",
        "                loss = lam2*criterion(output.squeeze(-1).float(), target.float())\n",
        "\n",
        "                loss.backward()                         # loss backwards\n",
        "                optimizer_disc.step()                        # update model params\n",
        "\n",
        "            # print('here')\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            op_temp = output.squeeze(-1).detach().cpu().numpy()\n",
        "            op_temp = (op_temp>0.5).astype(np.uint8)\n",
        "            # print(op_temp, target)\n",
        "\n",
        "            temp_train_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "            temp_train_acc2 += np.mean(op_temp==target_dash.detach().cpu().numpy())*data.size(0)\n",
        "            temp_train_acc3 += np.mean((output_est.detach().cpu().numpy()-output_dash.detach().cpu().numpy())**2)*data.size(0)\n",
        "            \n",
        "        \n",
        "        # validate-the-model\n",
        "        model.eval()\n",
        "        group_disc.eval()\n",
        "        temp_val_acc = 0.0\n",
        "        temp_val_acc2 = 0.0\n",
        "        temp_val_acc3 = 0.0\n",
        "        for data, target, grp in valid_loader:\n",
        "\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            grp = grp.to(device)\n",
        "\n",
        "            target_dash = torch.zeros((data.shape[0])).to(device)\n",
        "\n",
        "            ##Validation of the main model\n",
        "            output_dash = group_disc(data)                    # forward pass\n",
        "            output_dash = torch.reshape(output_dash,(output_dash.shape[0],output_dash.shape[1])).detach()\n",
        "\n",
        "            output_detangle, output_est = model_trans(output_dash)\n",
        "            output = model(output_detangle) \n",
        "\n",
        "            loss = criterion(output.squeeze(-1).float(), target.float())\n",
        "\n",
        "            # update-average-validation-loss \n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "            op_temp = output.squeeze(-1).detach().cpu().numpy()\n",
        "            op_temp = (op_temp>0.5).astype(np.uint8)\n",
        "\n",
        "            temp_val_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "            temp_val_acc2 += np.mean(op_temp==target_dash.detach().cpu().numpy())*data.size(0)\n",
        "            temp_val_acc3 += np.mean((output_est.detach().cpu().numpy()-output_dash.detach().cpu().numpy())**2)*data.size(0)\n",
        "\n",
        "        tvacc  = np.mean(np.array(temp_val_acc))\n",
        "\n",
        "        if tvacc>best_val_acc:\n",
        "            best_val_acc = tvacc\n",
        "            # torch.save(model.state_dict(), f\"checkpoints/\"+check_point_dir+\"/model_best.pt\")\n",
        "            print('Model saved')\n",
        "            write_file(text_path,'Model saved','a')\n",
        "\n",
        "        # calculate-average-losses\n",
        "        train_loss = train_loss/len(train_loader.sampler)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        \n",
        "        ttacc  = temp_train_acc/len(train_loader.sampler)\n",
        "        ttacc2  = temp_train_acc2/len(train_loader.sampler)\n",
        "        ttacc3  = temp_train_acc3/len(train_loader.sampler)\n",
        "\n",
        "        tvacc  = temp_val_acc/len(valid_loader.sampler)\n",
        "        tvacc2  = temp_val_acc2/len(valid_loader.sampler)\n",
        "        tvacc3  = temp_val_acc3/len(valid_loader.sampler)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        train_accuracies.append(ttacc)\n",
        "        val_accuracies.append(tvacc)\n",
        "\n",
        "\n",
        "        if flg==0: #this means run the generator updates\n",
        "            scheduler_trans.step()\n",
        "\n",
        "        if flg==1: #this means run the discriminator updates\n",
        "            scheduler_disc.step()\n",
        "\n",
        "        # print-training/validation-statistics \n",
        "        train_print = 'Epoch: {} \\tTr Loss: {:.3f} \\tTr Acc1: {:.3f}, \\t2: {:.3f}, \\t3: {:.3f} \\tVal Loss: {:.3f} \\tVal Acc1: {:.3f} \\t2: {:.3f} \\t3: {:.3f}'.format(\n",
        "            epoch, train_loss, ttacc, ttacc2, ttacc3, valid_loss, tvacc, tvacc2, tvacc3)\n",
        "        print(train_print)\n",
        "\n",
        "        test_print_D, ttacc_D = test_performance_trans(model,group_disc,model_trans,test_loader_D,criterion)\n",
        "        test_print_L, ttacc_L = test_performance_trans(model,group_disc,model_trans,test_loader_L,criterion)\n",
        "        \n",
        "        valid_accuracy.append(tvacc)\n",
        "        test_accuracy_D.append(ttacc_D)\n",
        "        test_accuracy_L.append(ttacc_L)\n",
        "\n",
        "        write_file(text_path,train_print,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(train_print)\n",
        "        \n",
        "        write_file(text_path,test_print_D,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(test_print_D)\n",
        "        \n",
        "        write_file(text_path,test_print_L,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(test_print_L)\n",
        "            \n",
        "    path_val = f\"checkpoints/\"+check_point_dir+\"/\"+\"validation_accuracy\"\n",
        "    path_D = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_D\"\n",
        "    path_L = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_L\"\n",
        "    valid_accuracy = np.array(valid_accuracy)\n",
        "    test_accuracy_D = np.array(test_accuracy_D)\n",
        "    test_accuracy_L = np.array(test_accuracy_L)\n",
        "    # np.save(path_val, valid_accuracy)\n",
        "    # np.save(path_D, test_accuracy_D)\n",
        "    # np.save(path_L, test_accuracy_L)\n",
        "\n",
        "    return model_trans, group_disc\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "79YqJaPVaYYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_fracs = [0.5]#np.linspace(0.0,1.0,11)\n",
        "for male_frac in male_fracs:\n",
        "    print('********* Male fraction: {} *********'.format(male_frac))\n",
        "    model_trans, group_disc = train_model_trans(male_frac,train_split_t,valid_loader,test_loader_D,test_loader_L)"
      ],
      "metadata": {
        "id": "EfawnrADadD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0994574-8c55-4a36-9eb4-0c3343ef6299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* Male fraction: 0.5 *********\n",
            "3512 1758 5616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.0000e-04.\n",
            "Adjusting learning rate of group 0 to 5.0000e-04.\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.9864e-04.\n",
            "Epoch: 1 \tTr Loss: 2.325 \tTr Acc1: 0.500, \t2: 0.397, \t3: 0.155 \tVal Loss: 0.693 \tVal Acc1: 0.507 \t2: 0.289 \t3: 0.443\n",
            "Test Loss: 0.784 \tTest Acc1: 0.554 \t2: 0.366 \t3: 0.442\n",
            "Test Loss: 0.775 \tTest Acc1: 0.478 \t2: 0.353 \t3: 0.430\n",
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.9459e-04.\n",
            "Epoch: 2 \tTr Loss: 2.263 \tTr Acc1: 0.482, \t2: 0.354, \t3: 0.104 \tVal Loss: 0.655 \tVal Acc1: 0.627 \t2: 0.320 \t3: 0.398\n",
            "Test Loss: 0.971 \tTest Acc1: 0.562 \t2: 0.312 \t3: 0.396\n",
            "Test Loss: 0.922 \tTest Acc1: 0.576 \t2: 0.344 \t3: 0.394\n",
            "Adjusting learning rate of group 0 to 4.8789e-04.\n",
            "Epoch: 3 \tTr Loss: 2.204 \tTr Acc1: 0.499, \t2: 0.347, \t3: 0.047 \tVal Loss: 0.695 \tVal Acc1: 0.492 \t2: 0.526 \t3: 0.516\n",
            "Test Loss: 0.703 \tTest Acc1: 0.424 \t2: 0.567 \t3: 0.513\n",
            "Test Loss: 0.690 \tTest Acc1: 0.554 \t2: 0.571 \t3: 0.510\n",
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.9864e-04.\n",
            "Epoch: 4 \tTr Loss: 1.369 \tTr Acc1: 0.784, \t2: 0.492, \t3: 0.515 \tVal Loss: 0.464 \tVal Acc1: 0.775 \t2: 0.517 \t3: 0.516\n",
            "Test Loss: 0.907 \tTest Acc1: 0.522 \t2: 0.513 \t3: 0.513\n",
            "Test Loss: 0.908 \tTest Acc1: 0.513 \t2: 0.531 \t3: 0.510\n",
            "Adjusting learning rate of group 0 to 4.9459e-04.\n",
            "Epoch: 5 \tTr Loss: 1.346 \tTr Acc1: 0.788, \t2: 0.501, \t3: 0.516 \tVal Loss: 0.470 \tVal Acc1: 0.768 \t2: 0.539 \t3: 0.516\n",
            "Test Loss: 0.932 \tTest Acc1: 0.531 \t2: 0.558 \t3: 0.513\n",
            "Test Loss: 0.909 \tTest Acc1: 0.536 \t2: 0.580 \t3: 0.510\n",
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.8789e-04.\n",
            "Epoch: 6 \tTr Loss: 1.299 \tTr Acc1: 0.794, \t2: 0.498, \t3: 0.515 \tVal Loss: 0.461 \tVal Acc1: 0.777 \t2: 0.427 \t3: 0.516\n",
            "Test Loss: 0.949 \tTest Acc1: 0.536 \t2: 0.402 \t3: 0.513\n",
            "Test Loss: 0.937 \tTest Acc1: 0.491 \t2: 0.446 \t3: 0.510\n",
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.7860e-04.\n",
            "Epoch: 7 \tTr Loss: 1.309 \tTr Acc1: 0.790, \t2: 0.503, \t3: 0.516 \tVal Loss: 0.443 \tVal Acc1: 0.781 \t2: 0.474 \t3: 0.516\n",
            "Test Loss: 0.887 \tTest Acc1: 0.549 \t2: 0.469 \t3: 0.513\n",
            "Test Loss: 0.869 \tTest Acc1: 0.531 \t2: 0.522 \t3: 0.510\n",
            "Adjusting learning rate of group 0 to 4.7860e-04.\n",
            "Epoch: 8 \tTr Loss: 2.403 \tTr Acc1: 0.524, \t2: 0.497, \t3: 0.055 \tVal Loss: 0.603 \tVal Acc1: 0.721 \t2: 0.483 \t3: 0.565\n",
            "Test Loss: 0.761 \tTest Acc1: 0.527 \t2: 0.411 \t3: 0.551\n",
            "Test Loss: 0.780 \tTest Acc1: 0.491 \t2: 0.446 \t3: 0.565\n",
            "Adjusting learning rate of group 0 to 4.6684e-04.\n",
            "Epoch: 9 \tTr Loss: 2.145 \tTr Acc1: 0.476, \t2: 0.503, \t3: 0.053 \tVal Loss: 0.677 \tVal Acc1: 0.611 \t2: 0.385 \t3: 0.395\n",
            "Test Loss: 0.708 \tTest Acc1: 0.545 \t2: 0.214 \t3: 0.370\n",
            "Test Loss: 0.714 \tTest Acc1: 0.424 \t2: 0.326 \t3: 0.395\n",
            "Adjusting learning rate of group 0 to 4.5273e-04.\n",
            "Epoch: 10 \tTr Loss: 2.136 \tTr Acc1: 0.496, \t2: 0.497, \t3: 0.047 \tVal Loss: 0.663 \tVal Acc1: 0.646 \t2: 0.493 \t3: 0.380\n",
            "Test Loss: 0.711 \tTest Acc1: 0.554 \t2: 0.348 \t3: 0.382\n",
            "Test Loss: 0.723 \tTest Acc1: 0.438 \t2: 0.420 \t3: 0.379\n",
            "Adjusting learning rate of group 0 to 4.3643e-04.\n",
            "Epoch: 11 \tTr Loss: 2.134 \tTr Acc1: 0.494, \t2: 0.501, \t3: 0.047 \tVal Loss: 0.682 \tVal Acc1: 0.601 \t2: 0.455 \t3: 0.259\n",
            "Test Loss: 0.706 \tTest Acc1: 0.545 \t2: 0.348 \t3: 0.274\n",
            "Test Loss: 0.718 \tTest Acc1: 0.429 \t2: 0.366 \t3: 0.255\n",
            "Adjusting learning rate of group 0 to 4.6684e-04.\n",
            "Epoch: 12 \tTr Loss: 1.498 \tTr Acc1: 0.757, \t2: 0.469, \t3: 0.263 \tVal Loss: 0.511 \tVal Acc1: 0.758 \t2: 0.451 \t3: 0.259\n",
            "Test Loss: 0.906 \tTest Acc1: 0.536 \t2: 0.482 \t3: 0.274\n",
            "Test Loss: 0.862 \tTest Acc1: 0.527 \t2: 0.455 \t3: 0.255\n",
            "Adjusting learning rate of group 0 to 4.5273e-04.\n",
            "Epoch: 13 \tTr Loss: 1.490 \tTr Acc1: 0.761, \t2: 0.479, \t3: 0.264 \tVal Loss: 0.510 \tVal Acc1: 0.760 \t2: 0.491 \t3: 0.259\n",
            "Test Loss: 0.882 \tTest Acc1: 0.562 \t2: 0.491 \t3: 0.274\n",
            "Test Loss: 0.861 \tTest Acc1: 0.531 \t2: 0.513 \t3: 0.255\n",
            "Adjusting learning rate of group 0 to 4.3643e-04.\n",
            "Epoch: 14 \tTr Loss: 1.443 \tTr Acc1: 0.777, \t2: 0.484, \t3: 0.264 \tVal Loss: 0.502 \tVal Acc1: 0.754 \t2: 0.526 \t3: 0.259\n",
            "Test Loss: 0.863 \tTest Acc1: 0.540 \t2: 0.522 \t3: 0.274\n",
            "Test Loss: 0.833 \tTest Acc1: 0.540 \t2: 0.540 \t3: 0.255\n",
            "Adjusting learning rate of group 0 to 4.1811e-04.\n",
            "Epoch: 15 \tTr Loss: 1.422 \tTr Acc1: 0.773, \t2: 0.490, \t3: 0.264 \tVal Loss: 0.500 \tVal Acc1: 0.761 \t2: 0.454 \t3: 0.259\n",
            "Test Loss: 0.884 \tTest Acc1: 0.545 \t2: 0.446 \t3: 0.274\n",
            "Test Loss: 0.827 \tTest Acc1: 0.549 \t2: 0.451 \t3: 0.255\n",
            "Adjusting learning rate of group 0 to 4.1811e-04.\n",
            "Epoch: 16 \tTr Loss: 2.341 \tTr Acc1: 0.572, \t2: 0.538, \t3: 0.045 \tVal Loss: 0.627 \tVal Acc1: 0.667 \t2: 0.351 \t3: 0.257\n",
            "Test Loss: 0.704 \tTest Acc1: 0.536 \t2: 0.402 \t3: 0.252\n",
            "Test Loss: 0.747 \tTest Acc1: 0.513 \t2: 0.344 \t3: 0.265\n",
            "Adjusting learning rate of group 0 to 3.9798e-04.\n",
            "Epoch: 17 \tTr Loss: 2.083 \tTr Acc1: 0.493, \t2: 0.595, \t3: 0.043 \tVal Loss: 0.718 \tVal Acc1: 0.478 \t2: 0.372 \t3: 0.183\n",
            "Test Loss: 0.692 \tTest Acc1: 0.558 \t2: 0.478 \t3: 0.195\n",
            "Test Loss: 0.737 \tTest Acc1: 0.424 \t2: 0.406 \t3: 0.184\n",
            "Adjusting learning rate of group 0 to 3.7625e-04.\n",
            "Epoch: 18 \tTr Loss: 2.075 \tTr Acc1: 0.502, \t2: 0.604, \t3: 0.041 \tVal Loss: 0.691 \tVal Acc1: 0.607 \t2: 0.308 \t3: 0.298\n",
            "Test Loss: 0.711 \tTest Acc1: 0.567 \t2: 0.397 \t3: 0.283\n",
            "Test Loss: 0.765 \tTest Acc1: 0.482 \t2: 0.312 \t3: 0.301\n",
            "Adjusting learning rate of group 0 to 3.5317e-04.\n",
            "Epoch: 19 \tTr Loss: 2.073 \tTr Acc1: 0.507, \t2: 0.611, \t3: 0.041 \tVal Loss: 0.625 \tVal Acc1: 0.709 \t2: 0.426 \t3: 0.207\n",
            "Test Loss: 0.702 \tTest Acc1: 0.536 \t2: 0.562 \t3: 0.209\n",
            "Test Loss: 0.729 \tTest Acc1: 0.478 \t2: 0.469 \t3: 0.210\n",
            "Adjusting learning rate of group 0 to 3.9798e-04.\n",
            "Epoch: 20 \tTr Loss: 1.558 \tTr Acc1: 0.749, \t2: 0.479, \t3: 0.206 \tVal Loss: 0.532 \tVal Acc1: 0.738 \t2: 0.435 \t3: 0.207\n",
            "Test Loss: 0.818 \tTest Acc1: 0.536 \t2: 0.482 \t3: 0.209\n",
            "Test Loss: 0.803 \tTest Acc1: 0.545 \t2: 0.464 \t3: 0.210\n",
            "Adjusting learning rate of group 0 to 3.7625e-04.\n",
            "Epoch: 21 \tTr Loss: 1.519 \tTr Acc1: 0.752, \t2: 0.471, \t3: 0.206 \tVal Loss: 0.511 \tVal Acc1: 0.759 \t2: 0.494 \t3: 0.207\n",
            "Test Loss: 0.852 \tTest Acc1: 0.522 \t2: 0.513 \t3: 0.209\n",
            "Test Loss: 0.842 \tTest Acc1: 0.527 \t2: 0.500 \t3: 0.210\n",
            "Adjusting learning rate of group 0 to 3.5317e-04.\n",
            "Epoch: 22 \tTr Loss: 1.526 \tTr Acc1: 0.750, \t2: 0.493, \t3: 0.207 \tVal Loss: 0.518 \tVal Acc1: 0.756 \t2: 0.462 \t3: 0.207\n",
            "Test Loss: 0.858 \tTest Acc1: 0.500 \t2: 0.491 \t3: 0.209\n",
            "Test Loss: 0.850 \tTest Acc1: 0.531 \t2: 0.469 \t3: 0.210\n",
            "Adjusting learning rate of group 0 to 3.2898e-04.\n",
            "Epoch: 23 \tTr Loss: 1.518 \tTr Acc1: 0.753, \t2: 0.482, \t3: 0.206 \tVal Loss: 0.511 \tVal Acc1: 0.759 \t2: 0.533 \t3: 0.207\n",
            "Test Loss: 0.839 \tTest Acc1: 0.545 \t2: 0.554 \t3: 0.209\n",
            "Test Loss: 0.832 \tTest Acc1: 0.527 \t2: 0.545 \t3: 0.210\n",
            "Adjusting learning rate of group 0 to 3.2898e-04.\n",
            "Epoch: 24 \tTr Loss: 2.184 \tTr Acc1: 0.566, \t2: 0.577, \t3: 0.040 \tVal Loss: 0.772 \tVal Acc1: 0.337 \t2: 0.571 \t3: 0.224\n",
            "Test Loss: 0.703 \tTest Acc1: 0.429 \t2: 0.634 \t3: 0.231\n",
            "Test Loss: 0.755 \tTest Acc1: 0.384 \t2: 0.562 \t3: 0.223\n",
            "Adjusting learning rate of group 0 to 3.0396e-04.\n",
            "Epoch: 25 \tTr Loss: 2.061 \tTr Acc1: 0.509, \t2: 0.645, \t3: 0.039 \tVal Loss: 0.680 \tVal Acc1: 0.565 \t2: 0.554 \t3: 0.205\n",
            "Test Loss: 0.680 \tTest Acc1: 0.558 \t2: 0.692 \t3: 0.210\n",
            "Test Loss: 0.738 \tTest Acc1: 0.455 \t2: 0.500 \t3: 0.205\n",
            "Adjusting learning rate of group 0 to 2.7837e-04.\n",
            "Epoch: 26 \tTr Loss: 2.056 \tTr Acc1: 0.488, \t2: 0.669, \t3: 0.037 \tVal Loss: 0.661 \tVal Acc1: 0.596 \t2: 0.552 \t3: 0.174\n",
            "Test Loss: 0.667 \tTest Acc1: 0.589 \t2: 0.661 \t3: 0.174\n",
            "Test Loss: 0.728 \tTest Acc1: 0.446 \t2: 0.464 \t3: 0.176\n",
            "Adjusting learning rate of group 0 to 2.5250e-04.\n",
            "Epoch: 27 \tTr Loss: 2.055 \tTr Acc1: 0.503, \t2: 0.683, \t3: 0.036 \tVal Loss: 0.692 \tVal Acc1: 0.522 \t2: 0.545 \t3: 0.170\n",
            "Test Loss: 0.667 \tTest Acc1: 0.571 \t2: 0.688 \t3: 0.175\n",
            "Test Loss: 0.730 \tTest Acc1: 0.442 \t2: 0.513 \t3: 0.171\n",
            "Adjusting learning rate of group 0 to 3.0396e-04.\n",
            "Epoch: 28 \tTr Loss: 1.508 \tTr Acc1: 0.758, \t2: 0.477, \t3: 0.169 \tVal Loss: 0.512 \tVal Acc1: 0.756 \t2: 0.448 \t3: 0.170\n",
            "Test Loss: 0.874 \tTest Acc1: 0.513 \t2: 0.478 \t3: 0.175\n",
            "Test Loss: 0.859 \tTest Acc1: 0.554 \t2: 0.455 \t3: 0.171\n",
            "Adjusting learning rate of group 0 to 2.7837e-04.\n",
            "Epoch: 29 \tTr Loss: 1.475 \tTr Acc1: 0.759, \t2: 0.473, \t3: 0.169 \tVal Loss: 0.505 \tVal Acc1: 0.756 \t2: 0.444 \t3: 0.170\n",
            "Test Loss: 0.889 \tTest Acc1: 0.509 \t2: 0.455 \t3: 0.175\n",
            "Test Loss: 0.853 \tTest Acc1: 0.545 \t2: 0.455 \t3: 0.171\n",
            "Adjusting learning rate of group 0 to 2.5250e-04.\n",
            "Epoch: 30 \tTr Loss: 1.460 \tTr Acc1: 0.763, \t2: 0.469, \t3: 0.169 \tVal Loss: 0.507 \tVal Acc1: 0.763 \t2: 0.510 \t3: 0.170\n",
            "Test Loss: 0.838 \tTest Acc1: 0.513 \t2: 0.540 \t3: 0.175\n",
            "Test Loss: 0.804 \tTest Acc1: 0.554 \t2: 0.545 \t3: 0.171\n",
            "Adjusting learning rate of group 0 to 2.2663e-04.\n",
            "Epoch: 31 \tTr Loss: 1.445 \tTr Acc1: 0.766, \t2: 0.491, \t3: 0.169 \tVal Loss: 0.503 \tVal Acc1: 0.738 \t2: 0.424 \t3: 0.170\n",
            "Test Loss: 0.873 \tTest Acc1: 0.513 \t2: 0.442 \t3: 0.175\n",
            "Test Loss: 0.843 \tTest Acc1: 0.549 \t2: 0.442 \t3: 0.171\n",
            "Adjusting learning rate of group 0 to 2.2663e-04.\n",
            "Epoch: 32 \tTr Loss: 2.223 \tTr Acc1: 0.560, \t2: 0.538, \t3: 0.044 \tVal Loss: 0.669 \tVal Acc1: 0.627 \t2: 0.337 \t3: 0.152\n",
            "Test Loss: 0.768 \tTest Acc1: 0.478 \t2: 0.379 \t3: 0.152\n",
            "Test Loss: 0.754 \tTest Acc1: 0.442 \t2: 0.451 \t3: 0.154\n",
            "Adjusting learning rate of group 0 to 2.0104e-04.\n",
            "Epoch: 33 \tTr Loss: 2.044 \tTr Acc1: 0.507, \t2: 0.636, \t3: 0.037 \tVal Loss: 0.683 \tVal Acc1: 0.556 \t2: 0.404 \t3: 0.128\n",
            "Test Loss: 0.745 \tTest Acc1: 0.411 \t2: 0.375 \t3: 0.128\n",
            "Test Loss: 0.753 \tTest Acc1: 0.366 \t2: 0.464 \t3: 0.129\n",
            "Adjusting learning rate of group 0 to 1.7602e-04.\n",
            "Epoch: 34 \tTr Loss: 2.038 \tTr Acc1: 0.488, \t2: 0.675, \t3: 0.035 \tVal Loss: 0.666 \tVal Acc1: 0.612 \t2: 0.471 \t3: 0.096\n",
            "Test Loss: 0.745 \tTest Acc1: 0.366 \t2: 0.509 \t3: 0.093\n",
            "Test Loss: 0.750 \tTest Acc1: 0.415 \t2: 0.567 \t3: 0.098\n",
            "Adjusting learning rate of group 0 to 1.5183e-04.\n",
            "Epoch: 35 \tTr Loss: 2.032 \tTr Acc1: 0.493, \t2: 0.704, \t3: 0.033 \tVal Loss: 0.690 \tVal Acc1: 0.543 \t2: 0.556 \t3: 0.084\n",
            "Test Loss: 0.754 \tTest Acc1: 0.362 \t2: 0.629 \t3: 0.086\n",
            "Test Loss: 0.759 \tTest Acc1: 0.312 \t2: 0.634 \t3: 0.086\n",
            "Adjusting learning rate of group 0 to 2.0104e-04.\n",
            "Epoch: 36 \tTr Loss: 1.489 \tTr Acc1: 0.762, \t2: 0.476, \t3: 0.084 \tVal Loss: 0.504 \tVal Acc1: 0.756 \t2: 0.433 \t3: 0.084\n",
            "Test Loss: 0.918 \tTest Acc1: 0.527 \t2: 0.429 \t3: 0.086\n",
            "Test Loss: 0.906 \tTest Acc1: 0.527 \t2: 0.482 \t3: 0.086\n",
            "Adjusting learning rate of group 0 to 1.7602e-04.\n",
            "Epoch: 37 \tTr Loss: 1.446 \tTr Acc1: 0.766, \t2: 0.483, \t3: 0.084 \tVal Loss: 0.507 \tVal Acc1: 0.757 \t2: 0.429 \t3: 0.084\n",
            "Test Loss: 0.884 \tTest Acc1: 0.509 \t2: 0.438 \t3: 0.086\n",
            "Test Loss: 0.857 \tTest Acc1: 0.531 \t2: 0.460 \t3: 0.086\n",
            "Adjusting learning rate of group 0 to 1.5183e-04.\n",
            "Epoch: 38 \tTr Loss: 1.445 \tTr Acc1: 0.773, \t2: 0.477, \t3: 0.084 \tVal Loss: 0.498 \tVal Acc1: 0.761 \t2: 0.463 \t3: 0.084\n",
            "Test Loss: 0.903 \tTest Acc1: 0.509 \t2: 0.455 \t3: 0.086\n",
            "Test Loss: 0.862 \tTest Acc1: 0.536 \t2: 0.500 \t3: 0.086\n",
            "Adjusting learning rate of group 0 to 1.2875e-04.\n",
            "Epoch: 39 \tTr Loss: 1.436 \tTr Acc1: 0.772, \t2: 0.498, \t3: 0.084 \tVal Loss: 0.494 \tVal Acc1: 0.766 \t2: 0.472 \t3: 0.084\n",
            "Test Loss: 0.878 \tTest Acc1: 0.504 \t2: 0.487 \t3: 0.086\n",
            "Test Loss: 0.839 \tTest Acc1: 0.522 \t2: 0.513 \t3: 0.086\n",
            "Adjusting learning rate of group 0 to 1.2875e-04.\n",
            "Epoch: 40 \tTr Loss: 2.364 \tTr Acc1: 0.572, \t2: 0.471, \t3: 0.034 \tVal Loss: 0.709 \tVal Acc1: 0.561 \t2: 0.854 \t3: 0.105\n",
            "Test Loss: 0.796 \tTest Acc1: 0.438 \t2: 0.821 \t3: 0.105\n",
            "Test Loss: 0.783 \tTest Acc1: 0.446 \t2: 0.821 \t3: 0.107\n",
            "Adjusting learning rate of group 0 to 1.0702e-04.\n",
            "Epoch: 41 \tTr Loss: 2.041 \tTr Acc1: 0.491, \t2: 0.616, \t3: 0.033 \tVal Loss: 0.758 \tVal Acc1: 0.485 \t2: 0.743 \t3: 0.059\n",
            "Test Loss: 0.764 \tTest Acc1: 0.375 \t2: 0.786 \t3: 0.065\n",
            "Test Loss: 0.773 \tTest Acc1: 0.326 \t2: 0.710 \t3: 0.061\n",
            "Adjusting learning rate of group 0 to 8.6890e-05.\n",
            "Epoch: 42 \tTr Loss: 2.035 \tTr Acc1: 0.511, \t2: 0.635, \t3: 0.032 \tVal Loss: 0.645 \tVal Acc1: 0.636 \t2: 0.669 \t3: 0.050\n",
            "Test Loss: 0.735 \tTest Acc1: 0.433 \t2: 0.728 \t3: 0.053\n",
            "Test Loss: 0.752 \tTest Acc1: 0.442 \t2: 0.638 \t3: 0.053\n",
            "Adjusting learning rate of group 0 to 6.8572e-05.\n",
            "Epoch: 43 \tTr Loss: 2.030 \tTr Acc1: 0.515, \t2: 0.664, \t3: 0.031 \tVal Loss: 0.661 \tVal Acc1: 0.575 \t2: 0.378 \t3: 0.045\n",
            "Test Loss: 0.748 \tTest Acc1: 0.375 \t2: 0.402 \t3: 0.052\n",
            "Test Loss: 0.753 \tTest Acc1: 0.353 \t2: 0.344 \t3: 0.048\n",
            "Adjusting learning rate of group 0 to 1.0702e-04.\n",
            "Epoch: 44 \tTr Loss: 1.552 \tTr Acc1: 0.743, \t2: 0.538, \t3: 0.046 \tVal Loss: 0.512 \tVal Acc1: 0.752 \t2: 0.456 \t3: 0.045\n",
            "Test Loss: 0.869 \tTest Acc1: 0.518 \t2: 0.455 \t3: 0.052\n",
            "Test Loss: 0.828 \tTest Acc1: 0.513 \t2: 0.478 \t3: 0.048\n",
            "Adjusting learning rate of group 0 to 8.6890e-05.\n",
            "Epoch: 45 \tTr Loss: 1.478 \tTr Acc1: 0.759, \t2: 0.515, \t3: 0.046 \tVal Loss: 0.514 \tVal Acc1: 0.757 \t2: 0.548 \t3: 0.045\n",
            "Test Loss: 0.882 \tTest Acc1: 0.518 \t2: 0.554 \t3: 0.052\n",
            "Test Loss: 0.867 \tTest Acc1: 0.522 \t2: 0.558 \t3: 0.048\n",
            "Adjusting learning rate of group 0 to 6.8572e-05.\n",
            "Epoch: 46 \tTr Loss: 1.497 \tTr Acc1: 0.757, \t2: 0.513, \t3: 0.046 \tVal Loss: 0.505 \tVal Acc1: 0.761 \t2: 0.485 \t3: 0.045\n",
            "Test Loss: 0.918 \tTest Acc1: 0.518 \t2: 0.491 \t3: 0.052\n",
            "Test Loss: 0.872 \tTest Acc1: 0.527 \t2: 0.500 \t3: 0.048\n",
            "Adjusting learning rate of group 0 to 5.2268e-05.\n",
            "Epoch: 47 \tTr Loss: 1.473 \tTr Acc1: 0.759, \t2: 0.497, \t3: 0.046 \tVal Loss: 0.508 \tVal Acc1: 0.758 \t2: 0.507 \t3: 0.045\n",
            "Test Loss: 0.864 \tTest Acc1: 0.518 \t2: 0.500 \t3: 0.052\n",
            "Test Loss: 0.827 \tTest Acc1: 0.536 \t2: 0.536 \t3: 0.048\n",
            "Adjusting learning rate of group 0 to 5.2268e-05.\n",
            "Epoch: 48 \tTr Loss: 2.228 \tTr Acc1: 0.573, \t2: 0.534, \t3: 0.032 \tVal Loss: 0.644 \tVal Acc1: 0.632 \t2: 0.365 \t3: 0.042\n",
            "Test Loss: 0.727 \tTest Acc1: 0.460 \t2: 0.344 \t3: 0.047\n",
            "Test Loss: 0.724 \tTest Acc1: 0.473 \t2: 0.321 \t3: 0.043\n",
            "Adjusting learning rate of group 0 to 3.8159e-05.\n",
            "Epoch: 49 \tTr Loss: 2.051 \tTr Acc1: 0.510, \t2: 0.605, \t3: 0.031 \tVal Loss: 0.681 \tVal Acc1: 0.559 \t2: 0.429 \t3: 0.034\n",
            "Test Loss: 0.738 \tTest Acc1: 0.415 \t2: 0.531 \t3: 0.040\n",
            "Test Loss: 0.724 \tTest Acc1: 0.424 \t2: 0.397 \t3: 0.036\n",
            "Adjusting learning rate of group 0 to 2.6397e-05.\n",
            "Epoch: 50 \tTr Loss: 2.047 \tTr Acc1: 0.511, \t2: 0.618, \t3: 0.030 \tVal Loss: 0.812 \tVal Acc1: 0.522 \t2: 0.493 \t3: 0.031\n",
            "Test Loss: 0.905 \tTest Acc1: 0.451 \t2: 0.522 \t3: 0.037\n",
            "Test Loss: 0.834 \tTest Acc1: 0.500 \t2: 0.509 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 1.7114e-05.\n",
            "Epoch: 51 \tTr Loss: 2.057 \tTr Acc1: 0.517, \t2: 0.568, \t3: 0.030 \tVal Loss: 0.685 \tVal Acc1: 0.539 \t2: 0.485 \t3: 0.030\n",
            "Test Loss: 0.735 \tTest Acc1: 0.388 \t2: 0.629 \t3: 0.037\n",
            "Test Loss: 0.727 \tTest Acc1: 0.402 \t2: 0.420 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 3.8159e-05.\n",
            "Epoch: 52 \tTr Loss: 1.573 \tTr Acc1: 0.733, \t2: 0.553, \t3: 0.031 \tVal Loss: 0.524 \tVal Acc1: 0.735 \t2: 0.498 \t3: 0.030\n",
            "Test Loss: 0.832 \tTest Acc1: 0.496 \t2: 0.504 \t3: 0.037\n",
            "Test Loss: 0.809 \tTest Acc1: 0.518 \t2: 0.491 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 2.6397e-05.\n",
            "Epoch: 53 \tTr Loss: 1.526 \tTr Acc1: 0.752, \t2: 0.503, \t3: 0.031 \tVal Loss: 0.517 \tVal Acc1: 0.754 \t2: 0.526 \t3: 0.030\n",
            "Test Loss: 0.895 \tTest Acc1: 0.509 \t2: 0.527 \t3: 0.037\n",
            "Test Loss: 0.861 \tTest Acc1: 0.531 \t2: 0.540 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 1.7114e-05.\n",
            "Epoch: 54 \tTr Loss: 1.509 \tTr Acc1: 0.749, \t2: 0.513, \t3: 0.031 \tVal Loss: 0.517 \tVal Acc1: 0.747 \t2: 0.525 \t3: 0.030\n",
            "Test Loss: 0.852 \tTest Acc1: 0.500 \t2: 0.491 \t3: 0.037\n",
            "Test Loss: 0.812 \tTest Acc1: 0.549 \t2: 0.540 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 1.0408e-05.\n",
            "Epoch: 55 \tTr Loss: 1.501 \tTr Acc1: 0.755, \t2: 0.511, \t3: 0.031 \tVal Loss: 0.524 \tVal Acc1: 0.740 \t2: 0.547 \t3: 0.030\n",
            "Test Loss: 0.840 \tTest Acc1: 0.496 \t2: 0.513 \t3: 0.037\n",
            "Test Loss: 0.805 \tTest Acc1: 0.540 \t2: 0.549 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 1.0408e-05.\n",
            "Epoch: 56 \tTr Loss: 2.443 \tTr Acc1: 0.630, \t2: 0.464, \t3: 0.031 \tVal Loss: 0.661 \tVal Acc1: 0.598 \t2: 0.712 \t3: 0.031\n",
            "Test Loss: 0.739 \tTest Acc1: 0.464 \t2: 0.732 \t3: 0.036\n",
            "Test Loss: 0.694 \tTest Acc1: 0.531 \t2: 0.683 \t3: 0.033\n",
            "Adjusting learning rate of group 0 to 6.3558e-06.\n",
            "Epoch: 57 \tTr Loss: 2.075 \tTr Acc1: 0.504, \t2: 0.548, \t3: 0.030 \tVal Loss: 0.728 \tVal Acc1: 0.485 \t2: 0.647 \t3: 0.030\n",
            "Test Loss: 0.733 \tTest Acc1: 0.433 \t2: 0.737 \t3: 0.035\n",
            "Test Loss: 0.703 \tTest Acc1: 0.464 \t2: 0.589 \t3: 0.032\n",
            "Adjusting learning rate of group 0 to 5.0000e-06.\n",
            "Epoch: 58 \tTr Loss: 2.070 \tTr Acc1: 0.482, \t2: 0.558, \t3: 0.030 \tVal Loss: 0.742 \tVal Acc1: 0.478 \t2: 0.646 \t3: 0.029\n",
            "Test Loss: 0.731 \tTest Acc1: 0.455 \t2: 0.750 \t3: 0.035\n",
            "Test Loss: 0.708 \tTest Acc1: 0.500 \t2: 0.616 \t3: 0.031\n",
            "Adjusting learning rate of group 0 to 6.3558e-06.\n",
            "Epoch: 59 \tTr Loss: 2.065 \tTr Acc1: 0.491, \t2: 0.560, \t3: 0.029 \tVal Loss: 0.760 \tVal Acc1: 0.450 \t2: 0.723 \t3: 0.029\n",
            "Test Loss: 0.745 \tTest Acc1: 0.366 \t2: 0.741 \t3: 0.035\n",
            "Test Loss: 0.711 \tTest Acc1: 0.522 \t2: 0.692 \t3: 0.031\n",
            "Adjusting learning rate of group 0 to 6.3558e-06.\n",
            "Epoch: 60 \tTr Loss: 1.619 \tTr Acc1: 0.728, \t2: 0.544, \t3: 0.030 \tVal Loss: 0.540 \tVal Acc1: 0.735 \t2: 0.554 \t3: 0.029\n",
            "Test Loss: 0.818 \tTest Acc1: 0.482 \t2: 0.545 \t3: 0.035\n",
            "Test Loss: 0.774 \tTest Acc1: 0.549 \t2: 0.585 \t3: 0.031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_performance(model,dataL,criterion):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    temp_test_acc = []\n",
        "\n",
        "    for data, target, grp in dataL:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        grp = grp.to(device)\n",
        "\n",
        "        output,_,_ = model(data)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        # update-average-validation-loss \n",
        "        test_loss += loss.item() * data.size(0)\n",
        "\n",
        "        op_temp = output.detach().cpu().numpy()\n",
        "        op_temp = np.argmax(op_temp,axis=1)\n",
        "\n",
        "        test_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "\n",
        "    ttacc  = test_acc/len(dataL.sampler)\n",
        "    test_loss_M = test_loss/len(dataL.sampler)\n",
        "    \n",
        "    test_print = 'Test Loss: {:.3f} \\tTest Acc: {:.3f}'.format(\n",
        "        test_loss_M, ttacc)\n",
        "\n",
        "    print(test_print)\n",
        "    return test_print,ttacc"
      ],
      "metadata": {
        "id": "eAU73PyFaiBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_file(fname,string,act):\n",
        "    with open(fname, act) as text_file:\n",
        "        text_file.write(string+'\\n')\n",
        "\n",
        "###Combined iteration\n",
        "def train_model(dark_frac,train_split_t,valid_loader,test_loader_D,test_loader_L,model_trans,group_disc):\n",
        "\n",
        "    num_epochs = 60\n",
        "    num_classes = 2 \n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    check_point_dir = 'October_perceptual_features_best_contrastive_bias_add_'+str(dark_frac)\n",
        "    \n",
        "    if not os.path.isdir(f\"checkpoints/\"+check_point_dir):\n",
        "        os.makedirs(f\"checkpoints/\"+check_point_dir)\n",
        "        print(\"Output directory is created\")\n",
        "        \n",
        "    #make logger text file\n",
        "    text_path = f\"checkpoints/\"+check_point_dir+\"/\"+\"log.txt\"\n",
        "    try:\n",
        "        os.remove(text_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    \n",
        "    write_file(text_path,'********* Dark fraction: {} *********'.format(dark_frac),'a')\n",
        "    \n",
        "    train_split = resample_dataset_race(train_split_t,dark_frac)\n",
        "    \n",
        "    write_file(text_path,str(train_split['race'].value_counts()),'a')\n",
        "    \n",
        "    write_file(text_path,str(train_split['gender'].value_counts()),'a')\n",
        "    \n",
        "    #Dataloaders\n",
        "    train_data = UTKFaceDataset(train_split, data_path, train_transform )\n",
        "\n",
        "    train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "    # model = models.resnet34(pretrained=False)\n",
        "    # model.fc = nn.Linear(512, num_classes)\n",
        "    # model.load_state_dict(torch.load(f\"model_init_2class.pt\"))\n",
        "    # model.to(device)\n",
        "\n",
        "    model = Network1(D_out = num_classes,device=device)\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for param in group_disc.parameters():\n",
        "        param.requires_grad = False\n",
        "    group_disc.to(device)\n",
        "\n",
        "    for param in model_trans.parameters():\n",
        "        param.requires_grad = False\n",
        "    group_disc.to(device)\n",
        "\n",
        "    lam = 0.5\n",
        "    beta = 0.8\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=learning_rate, \n",
        "        betas=(0.5, 0.999), \n",
        "        weight_decay=0.08\n",
        "        )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=30, \n",
        "        eta_min=0.01 * learning_rate, verbose=True\n",
        "        )\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Actual training of model\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    valid_accuracy = []\n",
        "    test_accuracy_D = []\n",
        "    test_accuracy_L = []\n",
        "\n",
        "\n",
        "    print(\"Training model...\")\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        # keep track of train/val loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        # training the model\n",
        "        model.train()\n",
        "        temp_train_acc = 0.0\n",
        "        for data, target, grp in train_loader:\n",
        "\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            grp = grp.to(device)\n",
        "\n",
        "            optimizer.zero_grad()                   # init gradients to zeros   \n",
        "\n",
        "            #feature extractor\n",
        "            output_dash = group_disc(data)                    # forward pass\n",
        "            output_dash = torch.reshape(output_dash,(output_dash.shape[0],output_dash.shape[1])).detach()\n",
        "\n",
        "            #disentangle\n",
        "            output_detangle, _ = model_trans(output_dash)\n",
        "\n",
        "            output_detangle = output_detangle.detach()\n",
        "\n",
        "            output,_,x2 = model(data)                    # forward pass\n",
        "\n",
        "            x2 = torch.reshape(x2,(x2.shape[0],-1))\n",
        "            loss = criterion(output, target)+lam*(beta**(epoch-1))*threshold_contrastive_trans(x2,output_detangle)        # compute loss\n",
        "\n",
        "            loss.backward()                         # loss backwards\n",
        "            optimizer.step()                        # update model params\n",
        "\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            op_temp = output.detach().cpu().numpy()\n",
        "            op_temp = np.argmax(op_temp,axis=1)\n",
        "\n",
        "            temp_train_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "            \n",
        "        \n",
        "        # validate-the-model\n",
        "        model.eval()\n",
        "        temp_val_acc = 0.0\n",
        "        for data, target, grp in valid_loader:\n",
        "\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            grp = grp.to(device)\n",
        "\n",
        "            output,_,_ = model(data)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # update-average-validation-loss \n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "            op_temp = output.detach().cpu().numpy()\n",
        "            op_temp = np.argmax(op_temp,axis=1)\n",
        "\n",
        "            temp_val_acc += np.mean(op_temp==target.detach().cpu().numpy())*data.size(0)\n",
        "\n",
        "        tvacc  = np.mean(np.array(temp_val_acc))\n",
        "\n",
        "        if tvacc>best_val_acc:\n",
        "            best_val_acc = tvacc\n",
        "            torch.save(model.state_dict(), f\"checkpoints/\"+check_point_dir+\"/model_best.pt\")\n",
        "            print('Model saved')\n",
        "            write_file(text_path,'Model saved','a')\n",
        "\n",
        "        # calculate-average-losses\n",
        "        train_loss = train_loss/len(train_loader.sampler)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        \n",
        "        ttacc  = temp_train_acc/len(train_loader.sampler)\n",
        "        tvacc  = temp_val_acc/len(valid_loader.sampler)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        train_accuracies.append(ttacc)\n",
        "        val_accuracies.append(tvacc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # print-training/validation-statistics \n",
        "        train_print = 'Epoch: {} \\tTr Loss: {:.3f} \\tTr Acc: {:.3f} \\tVal Loss: {:.3f} \\tVal Acc: {:.3f}'.format(\n",
        "            epoch, train_loss, ttacc, valid_loss, tvacc)\n",
        "        print(train_print)\n",
        "\n",
        "        test_print_D, ttacc_D = test_performance(model,test_loader_D,criterion)\n",
        "        test_print_L, ttacc_L = test_performance(model,test_loader_L,criterion)\n",
        "        \n",
        "        valid_accuracy.append(tvacc)\n",
        "        test_accuracy_D.append(ttacc_D)\n",
        "        test_accuracy_L.append(ttacc_L)\n",
        "\n",
        "        write_file(text_path,train_print,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(train_print)\n",
        "        \n",
        "        write_file(text_path,test_print_D,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(test_print_D)\n",
        "        \n",
        "        write_file(text_path,test_print_L,'a')\n",
        "#         with open(text_path, \"w\") as text_file:\n",
        "#             text_file.write(test_print_L)\n",
        "            \n",
        "    path_val = f\"checkpoints/\"+check_point_dir+\"/\"+\"validation_accuracy\"\n",
        "    path_D = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_D\"\n",
        "    path_L = f\"checkpoints/\"+check_point_dir+\"/\"+\"test_accuracy_L\"\n",
        "    valid_accuracy = np.array(valid_accuracy)\n",
        "    test_accuracy_D = np.array(test_accuracy_D)\n",
        "    test_accuracy_L = np.array(test_accuracy_L)\n",
        "    np.save(path_val, valid_accuracy)\n",
        "    np.save(path_D, test_accuracy_D)\n",
        "    np.save(path_L, test_accuracy_L)"
      ],
      "metadata": {
        "id": "vueGNKwlait6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_fracs = [0.5]#np.linspace(0.0,1.0,11)\n",
        "for male_frac in male_fracs:\n",
        "    print('********* Male fraction: {} *********'.format(male_frac))\n",
        "    train_model(male_frac,train_split_t,valid_loader,test_loader_D,test_loader_L,model_trans,group_disc)"
      ],
      "metadata": {
        "id": "GDR_7gLqamdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "516237f0-3fdd-4140-8788-e85a8503bbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********* Male fraction: 0.5 *********\n",
            "3512 1758 5616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.0000e-04.\n",
            "Training model...\n",
            "Model saved\n",
            "Adjusting learning rate of group 0 to 4.9864e-04.\n",
            "Epoch: 1 \tTr Loss: inf \tTr Acc: 0.564 \tVal Loss: nan \tVal Acc: 0.500\n",
            "Test Loss: nan \tTest Acc: 0.500\n",
            "Test Loss: nan \tTest Acc: 0.500\n",
            "Adjusting learning rate of group 0 to 4.9459e-04.\n",
            "Epoch: 2 \tTr Loss: nan \tTr Acc: 0.500 \tVal Loss: nan \tVal Acc: 0.500\n",
            "Test Loss: nan \tTest Acc: 0.500\n",
            "Test Loss: nan \tTest Acc: 0.500\n",
            "Adjusting learning rate of group 0 to 4.8789e-04.\n",
            "Epoch: 3 \tTr Loss: nan \tTr Acc: 0.500 \tVal Loss: nan \tVal Acc: 0.500\n",
            "Test Loss: nan \tTest Acc: 0.500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b67a2ea4d89a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmale_frac\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmale_fracs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'********* Male fraction: {} *********'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmale_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmale_frac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_split_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_L\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup_disc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-f830c5d188b2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dark_frac, train_split_t, valid_loader, test_loader_D, test_loader_L, model_trans, group_disc)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mtest_print_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttacc_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mtest_print_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttacc_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader_L\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvacc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-4a1912731585>\u001b[0m in \u001b[0;36mtest_performance\u001b[0;34m(model, dataL, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# update-average-validation-loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mop_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}